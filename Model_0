"""
# PyTorch Workflow
Let's explore an example PyTorch end-to-end workflow.
"""

what_is_covered = {1: "data (prepare and load)",
                   2: "build model",
                   3: "fitting the model to data (training)",
                   4: "making predictions and evaluating a model (inference)",
                   5: "saving and loading a model",
                   6: "putting it all together "}

import torch
from torch import nn # nn contains all of PyTorch's building blocks for neural networks
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

"""
## 1. Data (preparing and loading)

Data can be almost anything... in machine learning

* Excel spreadsheet
* Images of any kind
* Videos (YouTube has lots of data)
* Audio like songs or podcasts
* DNA
* Text

Machine learning is a game of two parts:
1. Get data into a numerical representation.
2. Build a model to learn patterns in that numerical representation.

To showcase this. let's create some *known* data using the linear regression formula.

We'll use a linear regression formula to make a straight line with *known* **parameters**.
"""

## Create *known* parameters
weights = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weights * X + bias

X[:10], y[:10]

len(X), len(y)

"""
Splitting data into training and test sets (one of the most important concepts in machine learning in general)

Let's create a training and test set with our data
How might we better visualize our data?
This is where the data explorer's motto comes in!

"Visualize, visualize, visualize!"
"""

# Create a train/test split (80/20)
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train), len(y_train), len(X_test), len(y_test)

def plot_predictions(train_data = X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels = y_test,
                     predictions = None):
    """ 
    Plot training data, test data and predictions.
    
    Args:
        train_data (torch.Tensor): Training features
        train_labels (torch.Tensor): Training targets
        test_data (torch.Tensor): Test features
        test_labels (torch.Tensor): Test targets
        predictions (torch.Tensor, optional): Model predictions on test data
        
    Returns:
        None: Displays a matplotlib plot
    """
    plt.figure(figsize=(10, 7))
    # Plot training data in blue
    plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")
    # Plot test data in green
    plt.scatter(test_data, test_labels, c="g", s=4, label= "Testing data")
    # Are there predictions?
    if predictions is not None:
        # Plot the predictions if they exist
        plt.scatter(test_data, predictions, c = "r", s=4, label="Predictions")

    # Show the legend
    plt.legend(prop={"size": 14})

plot_predictions()

"""
## 2. Build Model

What our model does:  
* Start with random values (weights & bias)
* Look at the training data and adjust the random values to better represent (or get closer to) the ideal values (the weights and bias we used to create the data)

How does it do so?

Through two main algorithms:
1. Gradient descent - https://www.youtube.com/watch?v=IHZwWFHWa-w
2. Backpropagation - https://www.youtube.com/watch?v=Ilg3gGewQ5U
"""

from torch import nn
# Create a linear progression model class
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherits from nn.Module
    """
    Linear regression model that learns weight and bias parameters.
  
    This model implements the equation: y = weight * x + bias
    where weight and bias are learnable parameters.
    """
    def __init__(self):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(1, # <- Start with a random weights and try to adjust it to the ideal weights
                                            requires_grad=True, # <- Can this parameter be updated via gradient descent?
                                            dtype=torch.float)) # <- PyTorch loves the datatype torch.float32

        self.bias = nn.Parameter(torch.randn(1, # <- Start with a random bias and try to adjust it to the ideal bias
                                         requires_grad=True, # <- Can this parameter be updated via gradient descent?
                                         dtype=torch.float)) # <- PyTorch loves the datatype torch.float32

    # Forward method to define the computation in the model
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass computation.
    
        Args:
            x (torch.Tensor): Input tensor
    
        Returns:
            torch.Tensor: Predicted output using linear function weight * x + bias
        """
        return self.weights * x + self.bias  # This is the linear regression formula

"""
### PyTorch model building essentials

* torch.nn - Contains all of the building blocks for computational graphs (a neural network can be considered a computational graph)
* torch.nn.Parameter - What parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us
* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()
* torch.nn.optim - This is where the optimizers in PyTorch live. They will help with gradient descent
* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation.

Essential modules PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html?ref=hackajob-insider
"""

# Create a random seed(42)
torch.manual_seed(42)

# Create an instance of the model (this is a subclass of nn.Module)
model_0 = LinearRegressionModel()

# Check out the parameters
list(model_0.parameters())

# List named parameters
model_0.state_dict()

weights, bias

X_test, y_test

"""
### Making prediction using `torch.inference_mode()`

To check out model's predictive power, let's see how well it predicts y_test based on X_test.

When we pass our data through our model, it's going to run it through the forward() method.
"""

# Make predictions with model
with torch.inference_mode():
    y_preds = model_0(X_test)

# You can do something similar with torch.no_grad(), however inference mode is preferred.

y_preds

y_test

plot_predictions(predictions=y_preds)

"""
## 3. Train model

The whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters.
Or in other words: From a poor representation of the data to a better representation of the data.

One way to measure how poor or how wrong your models predictions are is to use a loss function.

* Note: Loss function may also be called cost function or criterion in different areas. For our case we are going to refer to it as a loss function

Things we need to train:

* **Loss function:** A function to measure how wrong your models predictions are compared to the ideal outputs, lower is better.
* **Optimizer:** Takes into account the loss of a model and adjusts the models parameters (e.g. weights and biases) to improve the loss function. - https://pytorch.org/docs/stable/optim.html

* Inside the optimizer you'll often have to set two parameters:
  * `params` - The model parameters you'd like to optimize, for example `params = model_0.parameters()`
  * `lr` (learning rate) - The learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a large lr results in large changes)
 
And specifically for PyTorch, we need:
* A training loop
* A testing loop
"""

# Setup a loss function
loss_fn = nn.L1Loss()

# Setup an optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr = 0.1) # lr = learning rate = possibly the most important hyperparameter you can set

"""
**Q**: Which loss function and optimizer should I use?

**A**: This will be problem specific.

For example, for a regression problem (such as this), a loss function of `nn.L1Loss()` and an optimizer like `torch.optimizer.SGD()` will suffice.

But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss).

### Building a training loop (and a testing loop) in PyTorch

A couple of things we need in a training loop:

0. Loop through the data
1. Forward pass (This involves data moving through our models `forward()` functions) to make predictions on data - also called forward propagation.
2. Calculate the loss (compared forward pass predictions to ground truth labels)
3. Optimizer zero grad
4. Loss backward - Move backwards through the network to calculate the gradients of each of the parameters of our models with respect to the loss (**Back propagation**).
5. Optimizer step - Use the optimizer to adjust our models parameters to try and improve the loss (**Gradient Descent**).
"""

list(model_0.parameters())
X_test, y_test

# Set random seed for reproducibility
torch.manual_seed(42)

# An epoch is 1 loop through the data... (this is a hyperparameter because we've set it ourselves)
epochs = 100

"""
Train the PyTorch model.

Args:
    model: PyTorch model to train
    X_train (torch.Tensor): Training features
    y_train (torch.Tensor): Training targets
    X_test (torch.Tensor): Test features
    y_test (torch.Tensor): Test targets
    epochs (int): Number of complete passes through the training dataset
    lr (float): Learning rate for the optimizer

Returns:
    model: Trained PyTorch model
    losses: Dictionary containing training and test losses over epochs
"""
### Training

# Pass the data through the model for a number of epochs (e.g. 100)
for epoch in range(epochs):

    # 0. Put the model in training mode (this is the default state of the model)
    model_0.train()

    # 1. Forward pass on train data using the forward() method inside
    y_pred = model_0(X_train)

    # 2. Calculate the loss (how different are the model's predictions to the true values)
    loss = loss_fn(y_pred, y_train)

    # 3. Optimizer zero grad - Zero the gradients of the optimizer (they accumulate by default)
    optimizer.zero_grad()

    # 4. Perform back propagation on the loss with respect to the parameters of the model
    loss.backward()

    # 5. Progress/Step the optimizer (perform gradient descent)
    optimizer.step() # By default how the optimizer changes will accumulate through the loops so... we have to zero them above in step 3 for the next iteration of the loop.

    ### Testing
    model_0.eval() # Turns off gradient tracking/Turns off different settings in the model. Not needed for evaluation/testing (dropout/batch norm layers)

    with torch.inference_mode(): # Turns off gradient tracking & a couple more things behind the scenes - https://x.com/PyTorch/status/1437838231505096708?lang=en
    # with torch.no_grad(): # You may also see torch.no_grad() in older PyTorch code
        # 1. Do the forward pass
        test_pred = model_0(X_test)

        # 2. Calculate the test loss
        test_loss = loss_fn(test_pred, y_test)

    # Print out what's happening
    if epoch % 10 == 0:
        print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}")
        # Print out model state_dict()
        print(model_0.state_dict())

# Make predictions with trained model
with torch.inference_mode():
    y_predictions_new = model_0(X_test)

# Show trained model parameters
model_0.state_dict()

# Show original parameters used to create the data
print(f"Original parameters - weight: {weights}, bias: {bias}")

# Plot the original predictions
plot_predictions(predictions=y_preds)

# Plot the new predictions from trained model
plot_predictions(predictions=y_predictions_new)
